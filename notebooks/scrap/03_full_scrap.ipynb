{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Full scrap (>= 2025) et export brut v2\n",
    "Scrape toutes les games filtrées par date pour les équipes ciblées, récupère les détails complets et enregistre au format brut compatible (`json` + `pairedItem`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /home/ju/Documents/Dev/Dota-Datas\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def _find_root():\n",
    "    candidates = [Path.cwd()] + list(Path.cwd().parents[:4])\n",
    "    for cand in candidates:\n",
    "        if (cand / \"src\").exists():\n",
    "            return cand\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = _find_root()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "print(f\"Using project root: {ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aed6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teams: 0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.dota_data.api import (\n",
    "    load_api_key,\n",
    "    build_session,\n",
    "    load_team_list,\n",
    "    fetch_team_matches,\n",
    "    filter_matches_since,\n",
    "    unique_match_ids,\n",
    "    annotate_matches_with_team,\n",
    "    fetch_matches_chunked,\n",
    "    wrap_raw_match,\n",
    "    write_json,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "api_key = load_api_key(load_env_file=False)\n",
    "session = build_session(api_key)\n",
    "teams = load_team_list(ROOT / \"data/teams_to_look.csv\")\n",
    "print(f\"Teams: {len(teams)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a33f0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches team-level chargés: 173 entrées\n"
     ]
    }
   ],
   "source": [
    "# Charger les matches team-level depuis l'étape 01 si dispo\n",
    "raw_path = Path(\"data/interim/team_matches_raw.json\")\n",
    "if raw_path.exists():\n",
    "    team_matches = json.loads(raw_path.read_text())\n",
    "else:\n",
    "    team_matches = []\n",
    "    for team in teams:\n",
    "        team_matches.append({\"team\": team, \"matches\": fetch_team_matches(team[\"TeamID\"], session=session)})\n",
    "    write_json(team_matches, raw_path)\n",
    "print(f\"Matches team-level chargés: {len(team_matches)} entrées\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7f2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches >= 2025-01-01 (rows): 23051, ids uniques: 14158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8572758153, 8572646777, 8572546517, 8571364449, 8571247569]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_rows = []\n",
    "for entry in team_matches:\n",
    "    flat_rows.extend(annotate_matches_with_team(entry[\"matches\"], entry[\"team\"][\"TeamID\"], entry[\"team\"][\"TeamName\"]))\n",
    "matches_df = pd.DataFrame(flat_rows)\n",
    "cutoff_ts = int(datetime(2024, 1, 1, tzinfo=timezone.utc).timestamp())\n",
    "recent_rows = filter_matches_since(flat_rows, cutoff_ts)\n",
    "recent_ids = unique_match_ids(recent_rows)\n",
    "print(f\"Matches >= 2025-01-01 (rows): {len(recent_rows)}, ids uniques: {len(recent_ids)}\")\n",
    "recent_ids[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chunk 108/142] fetching 100 matches (ids 8115067937..8238800918)\n",
      "[chunk 108/142] saved 100 matches -> matches_v2_0107.json\n",
      "[chunk 109/142] fetching 100 matches (ids 8236639459..8509488345)\n",
      "[chunk 109/142] saved 100 matches -> matches_v2_0108.json\n",
      "[chunk 110/142] fetching 100 matches (ids 8509402374..7569457300)\n",
      "[chunk 110/142] saved 100 matches -> matches_v2_0109.json\n",
      "[chunk 111/142] fetching 100 matches (ids 7569040773..7826937718)\n",
      "[chunk 111/142] saved 100 matches -> matches_v2_0110.json\n",
      "[chunk 112/142] fetching 100 matches (ids 7826869189..7639413823)\n",
      "[chunk 112/142] saved 100 matches -> matches_v2_0111.json\n",
      "[chunk 113/142] fetching 100 matches (ids 7636204224..8171532827)\n",
      "[chunk 113/142] saved 100 matches -> matches_v2_0112.json\n",
      "[chunk 114/142] fetching 100 matches (ids 8171498115..8403202259)\n",
      "[chunk 114/142] saved 100 matches -> matches_v2_0113.json\n",
      "[chunk 115/142] fetching 100 matches (ids 8402021057..8186803129)\n",
      "[chunk 115/142] saved 100 matches -> matches_v2_0114.json\n",
      "[chunk 116/142] fetching 100 matches (ids 8186758302..8513029774)\n",
      "[chunk 116/142] saved 100 matches -> matches_v2_0115.json\n",
      "[chunk 117/142] fetching 100 matches (ids 8511656939..8249899514)\n",
      "[chunk 117/142] saved 100 matches -> matches_v2_0116.json\n",
      "[chunk 118/142] fetching 100 matches (ids 8248778169..7965693012)\n",
      "[chunk 118/142] saved 100 matches -> matches_v2_0117.json\n",
      "[chunk 119/142] fetching 100 matches (ids 7872142920..8094378250)\n",
      "[chunk 119/142] saved 100 matches -> matches_v2_0118.json\n",
      "[chunk 120/142] fetching 100 matches (ids 8094316677..7774706149)\n",
      "[chunk 120/142] saved 100 matches -> matches_v2_0119.json\n",
      "[chunk 121/142] fetching 100 matches (ids 7772758447..8057024084)\n",
      "[chunk 121/142] saved 99 matches -> matches_v2_0120.json\n",
      "[chunk 122/142] fetching 100 matches (ids 8056970364..7817902672)\n",
      "[chunk 122/142] saved 100 matches -> matches_v2_0121.json\n",
      "[chunk 123/142] fetching 100 matches (ids 7813689169..7633124191)\n",
      "[chunk 123/142] saved 100 matches -> matches_v2_0122.json\n",
      "[chunk 124/142] fetching 100 matches (ids 7633078798..8238472469)\n",
      "[chunk 124/142] saved 100 matches -> matches_v2_0123.json\n",
      "[chunk 125/142] fetching 100 matches (ids 8238415980..8406176818)\n",
      "[chunk 125/142] saved 100 matches -> matches_v2_0124.json\n",
      "[chunk 126/142] fetching 100 matches (ids 8404812666..7883017441)\n",
      "[chunk 126/142] saved 100 matches -> matches_v2_0125.json\n",
      "[chunk 127/142] fetching 100 matches (ids 7882922474..8443782266)\n",
      "[chunk 127/142] saved 100 matches -> matches_v2_0126.json\n",
      "[chunk 128/142] fetching 100 matches (ids 8443734872..8176267394)\n",
      "[chunk 128/142] saved 99 matches -> matches_v2_0127.json\n",
      "[chunk 129/142] fetching 100 matches (ids 8170383827..8273916461)\n",
      "[chunk 129/142] saved 100 matches -> matches_v2_0128.json\n",
      "[chunk 130/142] fetching 100 matches (ids 8273838990..8306810719)\n",
      "[chunk 130/142] saved 100 matches -> matches_v2_0129.json\n",
      "[chunk 131/142] fetching 100 matches (ids 8302747590..8364655296)\n",
      "[chunk 131/142] saved 100 matches -> matches_v2_0130.json\n",
      "[chunk 132/142] fetching 100 matches (ids 8360570539..8534591157)\n",
      "[chunk 132/142] saved 100 matches -> matches_v2_0131.json\n",
      "[chunk 133/142] fetching 100 matches (ids 8524046386..8504048567)\n",
      "[chunk 133/142] saved 100 matches -> matches_v2_0132.json\n",
      "[chunk 134/142] fetching 100 matches (ids 8472691602..8401922796)\n",
      "[chunk 134/142] saved 100 matches -> matches_v2_0133.json\n",
      "[chunk 135/142] fetching 100 matches (ids 8401888822..8119886916)\n",
      "[chunk 135/142] saved 100 matches -> matches_v2_0134.json\n",
      "[chunk 136/142] fetching 100 matches (ids 8115563907..7915596654)\n",
      "[chunk 136/142] saved 100 matches -> matches_v2_0135.json\n",
      "[chunk 137/142] fetching 100 matches (ids 7915499607..8035234682)\n",
      "[chunk 137/142] saved 100 matches -> matches_v2_0136.json\n",
      "[chunk 138/142] fetching 100 matches (ids 8035166831..8183746750)\n",
      "[chunk 138/142] saved 100 matches -> matches_v2_0137.json\n",
      "[chunk 139/142] fetching 100 matches (ids 8183683390..8471132217)\n",
      "[chunk 139/142] saved 100 matches -> matches_v2_0138.json\n",
      "[chunk 140/142] fetching 100 matches (ids 8319953470..7653934775)\n",
      "[chunk 140/142] saved 100 matches -> matches_v2_0139.json\n",
      "[chunk 141/142] fetching 100 matches (ids 7653889468..7874025579)\n",
      "[chunk 141/142] saved 100 matches -> matches_v2_0140.json\n",
      "[chunk 142/142] fetching 58 matches (ids 7873936991..7529155687)\n",
      "[chunk 142/142] saved 58 matches -> matches_v2_0141.json\n",
      "[retry] retrying 2 failed match_ids\n",
      "[retry] saved 2 recovered matches -> matches_v2_retry.json\n",
      "Summary: {'total_ids': 14158, 'chunk_size': 100, 'resume': True, 'chunks_written': 35, 'skipped_chunks': 107, 'errors_initial': 2, 'errors_remaining': 0, 'retry_saved': 2, 'out_dir': 'data/raw/chunks_v2', 'prefix': 'matches_v2'}\n"
     ]
    }
   ],
   "source": [
    "# Récupération détaillée avec chunking (sleep=1s)\n",
    "CHUNK_SIZE = 100\n",
    "OUT_DIR = Path(\"data/raw/chunks_v2\")\n",
    "PREFIX = \"matches_v2\"\n",
    "RESUME = True\n",
    "\n",
    "summary = fetch_matches_chunked(\n",
    "    recent_ids,\n",
    "    session=session,\n",
    "    out_dir=OUT_DIR,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    resume=RESUME,\n",
    "    sleep=1.0,\n",
    "    timeout=60,\n",
    "    prefix=PREFIX,\n",
    "    retry_failed=True,\n",
    ")\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7741d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 14160 matches into data/raw/data_v2.json (4271.07 MB)\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(\"data/raw/chunks_v2\")\n",
    "PREFIX = \"matches_v2\"\n",
    "\n",
    "# Stream-merge chunks to avoid high memory usage\n",
    "chunk_files = sorted(OUT_DIR.glob(f\"{PREFIX}_*.json\"))\n",
    "retry_file = OUT_DIR / f\"{PREFIX}_retry.json\"\n",
    "if retry_file.exists():\n",
    "    chunk_files.append(retry_file)\n",
    "out_path = Path(\"data/raw/data_v2.json\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "count = 0\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"[\")\n",
    "    first = True\n",
    "    for cf in chunk_files:\n",
    "        try:\n",
    "            arr = json.loads(cf.read_text())\n",
    "        except Exception as exc:\n",
    "            print(f\"Skipping {cf} (error {exc})\")\n",
    "            continue\n",
    "        for item in arr:\n",
    "            if not first:\n",
    "                f.write(\",\")\n",
    "            json.dump(item, f)\n",
    "            first = False\n",
    "            count += 1\n",
    "    f.write(\"]\")\n",
    "print(f\"Merged {count} matches into {out_path} ({out_path.stat().st_size/1024/1024:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e55dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Champs top-level sample: ['version', 'match_id', 'draft_timings', 'teamfights', 'objectives', 'chat', 'radiant_gold_adv', 'radiant_xp_adv', 'pauses', 'cosmetics', 'players', 'leagueid', 'start_time', 'duration', 'series_id'] ... (total 56)\n",
      "Objectives présents: True\n"
     ]
    }
   ],
   "source": [
    "# Stats rapides pour log\n",
    "if raw_payload:\n",
    "    sample_keys = list(raw_payload[0]['json'].keys())\n",
    "    print(f\"Champs top-level sample: {sample_keys[:15]} ... (total {len(sample_keys)})\")\n",
    "    print(f\"Objectives présents: {bool(raw_payload[0]['json'].get('objectives'))}\")\n",
    "else:\n",
    "    print('Aucun match combiné à inspecter.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

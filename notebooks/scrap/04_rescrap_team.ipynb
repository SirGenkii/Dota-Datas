{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - One-shot rescrap for a single team\n",
    "\n",
    "Rescrape matches for one team, filtered since a target date, with retry and error log. Appends new matches to the existing `data/raw/data_v2.json` in the same raw format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74c84b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ROOT=/home/ju/Documents/Dev/Dota-Datas\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "def _find_root():\n",
    "    cand = Path.cwd()\n",
    "    for c in [cand, *cand.parents]:\n",
    "        if (c / \"src\").exists() and (c / \"data\").exists():\n",
    "            return c\n",
    "    return cand\n",
    "\n",
    "ROOT = _find_root()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "print(f\"Using ROOT={ROOT}\")\n",
    "\n",
    "from src.dota_data.api import (\n",
    "    load_api_key,\n",
    "    build_session,\n",
    "    fetch_team_matches,\n",
    "    filter_matches_since,\n",
    "    unique_match_ids,\n",
    "    fetch_match_details,\n",
    "    wrap_raw_match,\n",
    ")\n",
    "\n",
    "# Parameters\n",
    "TEAM_ID = 9572001  # change as needed\n",
    "MIN_DATE = datetime(2024, 1, 1, tzinfo=timezone.utc)\n",
    "MIN_START_TIME = int(MIN_DATE.timestamp())\n",
    "RAW_PATH = ROOT / \"data\" / \"raw\" / \"data_v2.json\"\n",
    "ERROR_LOG = ROOT / \"data\" / \"interim\" / f\"rescrap_errors_team_{TEAM_ID}.json\"\n",
    "\n",
    "RAW_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "ERROR_LOG.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff6560c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing raw entries: 14170\n",
      "Existing unique match_ids: 14170\n"
     ]
    }
   ],
   "source": [
    "# Load existing raw data\n",
    "if RAW_PATH.exists():\n",
    "    raw_data = json.loads(RAW_PATH.read_text())\n",
    "    print(f\"Loaded existing raw entries: {len(raw_data)}\")\n",
    "else:\n",
    "    raw_data = []\n",
    "    print(\"No existing raw file, starting fresh\")\n",
    "\n",
    "existing_ids = OrderedDict()\n",
    "for item in raw_data:\n",
    "    mid = item.get(\"json\", {}).get(\"match_id\")\n",
    "    if isinstance(mid, int):\n",
    "        existing_ids[mid] = item\n",
    "print(f\"Existing unique match_ids: {len(existing_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9af154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ready\n",
      "Team matches since 2024-01-01: 321\n",
      "New match_ids to fetch (not already in raw): 12\n"
     ]
    }
   ],
   "source": [
    "# Build session\n",
    "api_key = load_api_key()\n",
    "session = build_session(api_key)\n",
    "print(\"Session ready\")\n",
    "\n",
    "# Fetch team matches and filter\n",
    "matches = fetch_team_matches(TEAM_ID, session=session)\n",
    "matches = filter_matches_since(matches, MIN_START_TIME)\n",
    "print(f\"Team matches since {MIN_DATE.date()}: {len(matches)}\")\n",
    "\n",
    "new_match_ids = [mid for mid in unique_match_ids(matches) if mid not in existing_ids]\n",
    "print(f\"New match_ids to fetch (not already in raw): {len(new_match_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3865285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: fetching 12 matches\n",
      "All match details fetched\n"
     ]
    }
   ],
   "source": [
    "# Fetch details with retries\n",
    "MAX_RETRIES = 3\n",
    "SLEEP_BETWEEN = 1.0\n",
    "errors = []\n",
    "details = []\n",
    "remaining = list(new_match_ids)\n",
    "for attempt in range(1, MAX_RETRIES + 1):\n",
    "    if not remaining:\n",
    "        break\n",
    "    print(f\"Attempt {attempt}: fetching {len(remaining)} matches\")\n",
    "    batch, err = fetch_match_details(remaining, session=session, sleep=SLEEP_BETWEEN)\n",
    "    details.extend(batch)\n",
    "    errors.extend(err)\n",
    "    remaining = [mid for mid, exc in err]\n",
    "    if remaining:\n",
    "        time.sleep(2)\n",
    "\n",
    "if remaining:\n",
    "    print(f\"Still failing after retries: {len(remaining)}\")\n",
    "else:\n",
    "    print(\"All match details fetched\")\n",
    "\n",
    "if errors:\n",
    "    ERROR_LOG.write_text(json.dumps([(mid, str(exc)) for mid, exc in errors], indent=2))\n",
    "    print(f\"Logged errors to {ERROR_LOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8074c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped new matches: 12\n",
      "Existing entries: 14158\n",
      "New entries: 12\n",
      "Wrote updated raw to /home/ju/Documents/Dev/Dota-Datas/data/raw/data_v2.json (entries=14170)\n"
     ]
    }
   ],
   "source": [
    "# Wrap and merge (streamed to avoid large RAM usage)\n",
    "wrapped = [wrap_raw_match(d) for d in details if isinstance(d, dict) and d.get(\"match_id\")]\n",
    "print(f\"Wrapped new matches: {len(wrapped)}\")\n",
    "\n",
    "out_path = RAW_PATH\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "count_existing = len(existing_ids)\n",
    "count_new = len(wrapped)\n",
    "print(f\"Existing entries: {count_existing}\")\n",
    "print(f\"New entries: {count_new}\")\n",
    "\n",
    "# Stream write: start with existing, then append/overwrite with new\n",
    "combined = existing_ids.copy()\n",
    "for item in wrapped:\n",
    "    mid = item[\"json\"].get(\"match_id\")\n",
    "    combined[mid] = item\n",
    "\n",
    "with out_path.open('w', encoding='utf-8') as f:\n",
    "    f.write('[')\n",
    "    first = True\n",
    "    for item in combined.values():\n",
    "        if not first:\n",
    "            f.write(',')\n",
    "        f.write(json.dumps(item))\n",
    "        first = False\n",
    "    f.write(']')\n",
    "print(f\"Wrote updated raw to {out_path} (entries={len(combined)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Existing entries before merge\n",
    "- Matches fetched since 2024-01-01\n",
    "- New details fetched and merged\n",
    "- Errors logged (if any) to interim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
